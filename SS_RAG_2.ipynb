{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2:\n",
        "### Interactive QA Bot Interface\n",
        "#### Problem Statement:\n",
        "#### Develop an interactive interface for the QA bot from Part 1, allowing users to input queries and retrieve answers in real time. The interface should enable users to upload documents and ask questions based on the content of the uploaded document.\n",
        "Task Requirements:\n",
        "1. Build a simple frontend interface using Streamlit or Gradio, allowing users to\n",
        "upload PDF documents and ask questions.\n",
        "2. Integrate the backend from Part 1 to process the PDF, store document embeddings,\n",
        "and provide real-time answers to user queries.\n",
        "3. Ensure that the system can handle multiple queries efficiently and provide accurate,\n",
        "contextually relevant responses.\n",
        "4. Allow users to see the retrieved document segments alongside the generated\n",
        "answer.\n"
      ],
      "metadata": {
        "id": "_JtkC-WXhPuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and Requirements\n",
        "!pip install -q streamlit PyPDF2 pinecone-client cohere transformers torch"
      ],
      "metadata": {
        "id": "Np4K5faQ1HDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "459a5461-1ae9-45e6-c0b7-068c382a2543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.1/233.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!wget -q -O - ipv4.icanhazip.com # xx.xxx.xxx.xxx    this ipv4 to be used as password for npx localtunnel port 8501"
      ],
      "metadata": {
        "id": "SlsO9wIU_iXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# import libraries\n",
        "import PyPDF2\n",
        "import streamlit as st\n",
        "import pinecone\n",
        "import cohere\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Extract pdf\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    reader = PyPDF2.PdfReader(pdf_file)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(reader.pages)):\n",
        "        page = reader.pages[page_num]\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Initialize pinecone\n",
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "pc = Pinecone(api_key=\"axxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
        "\n",
        "# Reset index\n",
        "pc.delete_index('rag')\n",
        "\n",
        "# Create index\n",
        "if 'rag' not in pc.list_indexes().names():\n",
        "          pc.create_index(\n",
        "              name='rag',\n",
        "              dimension=384,\n",
        "              metric='cosine',\n",
        "              spec=ServerlessSpec(\n",
        "                  cloud='aws',\n",
        "                  region='us-east-1'\n",
        "              )\n",
        "          )\n",
        "\n",
        "# Vector Database Setup\n",
        "index = pc.Index('rag')\n",
        "\n",
        "# Initialize Cohere for text generation (alternatively, GPT-3/4 API can be used)\n",
        "cohere_client = cohere.Client(api_key=\"bxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
        "\n",
        "# Load a pre-trained embedding model from Hugging Face (e.g., sentence-transformers)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**inputs)\n",
        "    embeddings = model_output.last_hidden_state.mean(dim=1)  # Average pooling\n",
        "\n",
        "    # Convert to numpy array and cast to float32\n",
        "    embeddings_np = embeddings.numpy().astype(np.float32)\n",
        "\n",
        "    # L2 normalization (make sure the norm of the vector is 1)\n",
        "    norms = np.linalg.norm(embeddings_np, axis=1, keepdims=True)\n",
        "    normalized_embeddings = embeddings_np / norms  # Apply L2 normalization\n",
        "\n",
        "    return normalized_embeddings\n",
        "\n",
        "# Streamlit App\n",
        "st.title(\"Interactive QA Bot with Document Upload\")\n",
        "\n",
        "# Step 1: Upload PDF\n",
        "uploaded_file = st.file_uploader(\"Upload a PDF document\", type=\"pdf\")\n",
        "if uploaded_file:\n",
        "    document_text = extract_text_from_pdf(uploaded_file)\n",
        "    st.write(\"Document uploaded successfully!\")\n",
        "\n",
        "    # Step 2: Generate embeddings and store in Pinecone\n",
        "    document_segments = document_text.split(\". \")  # Split the document into sentences\n",
        "    # Insert documents into Pinecone with their embeddings\n",
        "    for i, segment in enumerate(document_segments):\n",
        "          embedding = generate_embeddings([segment])[0].tolist()\n",
        "          index.upsert([(f\"doc_{i}\", embedding, {\"text\": segment})])  # Store the embedding in Pinecone\n",
        "\n",
        "    st.write(f\"Stored {len(document_segments)} document segments in Pinecone.\")\n",
        "\n",
        "# Step 3: Query Input\n",
        "query = st.text_input(\"Ask a question based on the document:\")\n",
        "if query and uploaded_file:\n",
        "    # Retrieve relevant document segments from Pinecone\n",
        "    def retrieve_relevant_docs(query, top_k=3):\n",
        "        query_embedding = generate_embeddings([query])[0].tolist()\n",
        "\n",
        "        results = index.query(vector=[query_embedding], top_k=top_k)\n",
        "        relevant_docs = []\n",
        "        if 'matches' in results and results['matches']:\n",
        "            for match in results['matches']:\n",
        "                doc_id = match['id']\n",
        "                doc_index = int(doc_id.split(\"_\")[1])  # Assuming \"doc_X\" format\n",
        "                relevant_docs.append(document_segments[doc_index])\n",
        "        else:\n",
        "              print(\"No matches found in the query results.\")\n",
        "        return relevant_docs\n",
        "\n",
        "    # Generate the answer using Cohere or any generative model\n",
        "    def generate_answer(query, relevant_docs):\n",
        "        context = \"\\n\".join(relevant_docs)\n",
        "        prompt = f\"Question: {query}\\n\\nContext:\\n{context}\\n\\nAnswer:\"\n",
        "        response = cohere_client.generate(\n",
        "            model=\"command-nightly\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=700,\n",
        "            temperature=0.5\n",
        "        )\n",
        "        return response.generations[0].text.strip()\n",
        "\n",
        "    # QA Bot Function\n",
        "    def qa_bot(query):\n",
        "        # Retrieve relevant documents based on the query\n",
        "        relevant_docs = retrieve_relevant_docs(query)\n",
        "\n",
        "        # Generate a coherent answer using Cohere\n",
        "        answer = generate_answer(query, relevant_docs)\n",
        "        return answer, relevant_docs\n",
        "\n",
        "    # Display the generated answer\n",
        "    answer, relevant_docs = qa_bot(query)\n",
        "    st.write(\"Answer:\")\n",
        "    st.write(answer)\n",
        "    st.write(\"Relevant Document Segments:\")\n",
        "    for doc in relevant_docs:\n",
        "        st.write(f\"- {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzEWAaYu1pXq",
        "outputId": "93df8e30-d1eb-4533-ec11-3f6db13eddb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joTHqLiKLS1g",
        "outputId": "909b06e3-ca84-4af5-a259-00b0bc2fac79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.204.10.61:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://early-tools-type.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}